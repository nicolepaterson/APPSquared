#!usr/bin/bash
# @nicolemariepaterson
# HA pipeline

#set -e
#set -u
set -o pipefail

timestamp=$(date +%Y-%m-%d)
echo $timestamp

#define args
#"./BCH relax" runs the relax pipeline, must be run first 
#"./BCH tables" generates the data tables
#"./BCH upload" sends table data to cdp and refreshes the relaxed_isolate_name table

#paths not expected to change frequently for input/output
out_p=
if [ "$out_p" == "" ]; then
    out_p=$(cd -P "$(dirname "${BASH_SOURCE[0]}")" && pwd)
fi
echo $out_p

date_output="$out_p/$timestamp"
relaxed_dir="$out_p/relaxed"
unrelaxed_dir="$out_p/unrelaxed"
home="$out_p"

if [ ! -d "$date_output" ]; then
    mkdir $date_output
fi
if [ ! -d "$relaxed_dir" ]; then
    mkdir $relaxed_dir
fi
if [ ! -d "$unrelaxed_dir" ]; then
    mkdir $unrelaxed_dir
fi
timestamp=$(date +%Y-%m-%d)

function move_files {
for file in $home/relaxed*.pdb
do
    cp $file $relaxed_dir
done

for file in $home/*.pdb
do
    cp $file $unrelaxed_dir
done

for file in $unrelaxed_dir/relaxed_*.pdb
do
    rm $file
done
}

function run_pdb_parse {
for file in $unrelaxed_dir/*.pdb
do
    filename=$(basename "$file")
    name=$(basename "$file" .pdb)
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/appsquared
    python pdb_parse_raw.py -i $file -o $date_output/$name"_pdb.tsv" -v $name -t $timestamp
done
}
function run_relax {
for file in $unrelaxed_dir/*.pdb
do
    filename=$(basename "$file")
    #module load rosetta/3.13-cluster
    #mpirun -np 2 -nooversubscribe relax.default.linuxgccrelease -quick -ex1 -ex2 -gpu true -relax:minimize_bondlength_subset=3 -in:file:s $file --out:pdb -out:prefix relaxed_ -out:path:pdb $pdb_dir -relax:fast
    mpirun -np 3 -nooversubscribe relax.cxx11threadmpiserialization.linuxgccrelease -overwrite -quick -ex1 -ex2 -gpu true -device 0 -multithreading true -total_threads 8 -relax:minimize_bondlength_subset=3 -in:file:s $file --out:pdb -out:prefix relaxed_quick_ -out:path:pdb $relaxed_dir -relax:fast
done
}
#run Rosetta energy score for ddG, prep file for cdp upload
function run_rosetta { 
for file in $relaxed_dir/relaxed_*.pdb
#for file in $home/relaxed_*.pdb
do
    filename=$(basename "$file" .pdb)
    #module load rosetta/3.13-cluster
    timestamp=$(date +%Y-%m-%d)
    score_jd2.default.linuxgccrelease -simple_metrics true -renumber_pdb -in:ignore_unrecognized_res -in:file:s $file -in:file:native $ref_path -out:file:scorefile $date_output/$filename.features.txt
    #mpirun score_jd2.mpi.linuxgccrelease -simple_metrics true -multithreading true -total_threads 4 -renumber_pdb -in:ignore_unrecognized_res -in:file:s $file -in:file:native $ref_path -out:file:scorefile $gwa_output/$filename.features.txt
    score_aln2.default.linuxgccrelease -multithreading true -total_threads 4 -in:file:s $file out:file:scorefile $date_output/$filename.aln2_scorefile.txt
    residue_energy_breakdown.linuxgccrelease -mp:quickrelax:repack_again -in:file:s $file -out:file:silent $date_output/$filename.res_breakdown.txt
done
}
function pdb_parquet_gen {
for file in $date_output/*pdb.tsv
do
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/appsquared
    conda activate --stack appsquared
    python3 tsv_to_parquet.py $file
done
}
#computes the glycosylation distances for each site from pdb file
function run_glyc_dist { 
for file in $relaxed_dir/relaxed_*.pdb
#for file in $home/relaxed_*.pdb
do
    filename=$(basename $file)
    name=$(basename $file .pdb)
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/glyc
    conda activate --stack glyc
    python glyc.py -i $file -v $name -t $timestamp -o $date_output/$name.ASA_glyc.csv
    sed -i "s/relaxed_quick_//g" $date_output/$name.ASA_glyc.csv
    sed -i "s/relaxed_//g" $date_output/$name.ASA_glyc.csv
    sed -i "s/_0001//g" $date_output/$name.ASA_glyc.csv
    python3 tsv_to_parquet.py $date_output/$name.ASA_glyc.csv
done
}
function run_get_contacts {
for file in $relaxed_dir/relaxed_*.pdb
#for file in $home/relaxed_*.pdb
do
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/getcontacts
    filename=$(basename $file)
    name=$(basename $file .pdb)
    python get_static_contacts.py --structure $filename --output $date_output/$name.static_contacts.csv --itypes all
done
}
#cleans up the getcontacts file for upload to CDP. Needs to be updated to parquet and moved to clean_up_for_tables
function run_get_contacts_clean {
for file in $date_output/*.static_contacts.csv
do
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/appsquared
    filename=$(basename "$file" .pdb)
    name=$(basename $file .static_contacts.csv)
    timestamp=$(date +%Y-%m-%d)
    sed -i '1d' $file
    sed -i '1d' $file
    sed -i 's/:/\t/g' $file
    python getcontacts_df.py -i $file -v $name -t $timestamp
done
for file in $date_output/*.getcontacts.csv
do
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/appsquared
    sed -i "s/relaxed_quick_//g" $file
    sed -i "s/relaxed_//g" $file
    sed -i "s/_0001//g" $file
    python3 tsv_to_parquet.py $file
done
}
function clean_up_for_tables {
for file in $date_output/*.res_breakdown.txt
do
    sed -i "s/SCORE:  //g" $file
    sed -i "s/relaxed_quick_//g" $file
    sed -i "s/relaxed_//g" $file
    sed -i "s/_0001//g" $file
    source ~/miniconda3/etc/profile.d/conda.sh
    source ~/miniconda3/bin/activate --stack ~/.conda/envs/appsquared
    conda activate --stack appsquared
    name=$(basename $file .res_breakdown.txt)
    python dataframe_per_res.py -i $file -v $name -t $timestamp
done
for file in $date_output/*.per_res_scorefile.tsv
do
    sed -i "s/relaxed_quick_//g" $file
    sed -i "s/relaxed_//g" $file
    sed -i "s/_0001//g" $file
    python3 tsv_to_parquet.py $file
done
}

if [ "$1" == "relax" ]; then
    #relax required for most pipeline applications
    move_files
    echo $timestamp "OPTIMIZING STRUCTURES"
    run_relax
fi

if [ "$1" == "tables" ]; then
    move_files
    echo $timestamp "GENERATING DATA FOR CDP TABLES"
    #Runs the pipeline in order
    echo $timestamp "Parsing raw pdb"
    #run_pdb_parse
    #pdb_parquet_gen
    echo $timestamp "Running Rosetta"
    run_rosetta 
    echo $timestamp "Running Glycosylation Distance Calculator"
    run_glyc_dist 
    echo $timestamp "Running GetContacts: Intermolecular Interactions"run_get_contacts 
    run_get_contacts_clean 
    echo $timestamp "Cleaning up the tables for database entry"
    clean_up_for_tables
fi
